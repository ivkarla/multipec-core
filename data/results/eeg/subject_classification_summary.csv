subject,f1_scores,auc_scores,class_accuracies,class_counts,feature_importances,f1,auc
1,"[0.5491909217804891, 0.5626712468817732, 0.6246360582306831, 0.5554496914222314, 0.5303258145363409]","[np.float64(0.9279732100218916), np.float64(0.9237953801730893), np.float64(0.9276550082471136), np.float64(0.9391788572529025), np.float64(0.8946257018599639)]","{np.str_('S1'): np.float64(0.5925925925925926), np.str_('S2'): np.float64(0.9393939393939394), np.str_('S3'): np.float64(0.8852459016393442), np.str_('S4'): np.float64(0.6666666666666666), np.str_('S5'): np.float64(0.25), np.str_('S6'): np.float64(0.0), np.str_('S7'): np.float64(0.030303030303030304)}","{np.str_('S1'): np.int64(27), np.str_('S2'): np.int64(33), np.str_('S3'): np.int64(122), np.str_('S4'): np.int64(15), np.str_('S5'): np.int64(28), np.str_('S6'): np.int64(28), np.str_('S7'): np.int64(33)}","{'spatial processing': np.float64(1.1225492160615487), 'sound localization': np.float64(0.9002579746972807), 'motor planning': np.float64(0.8944905264125401), 'object recognition': np.float64(0.7669545054376405), 'decision making': np.float64(0.7398649786833709), 'sensorimotor integration': np.float64(0.7335002693300857), 'visual spatial processing': np.float64(0.6925934250666621), 'speech perception': np.float64(0.6676223017403051), 'auditory processing': np.float64(0.6175497205359944), 'speech and language integration': np.float64(0.5235746785213207), 'spatial awareness': np.float64(0.49546819130701225), 'tactile processing': np.float64(0.43311756135866153), 'high-level visual processing': np.float64(0.409962445149431), 'language': np.float64(0.40966945250193537), 'primary auditory processing': np.float64(0.3773537670650117), 'auditory-motor integration': np.float64(0.36996855457553285), 'visual integration': np.float64(0.3671547453551173), 'motor cortex - movement of left limbs': np.float64(0.3179812482289361), 'working memory': np.float64(0.3111993829269015), 'attention': np.float64(0.2730810053239401), 'motor cortex - movement of right limbs': np.float64(0.270836976613069), 'visual attention': np.float64(0.2691247402055244), 'executive function': np.float64(0.2207047949390779), 'auditory association': np.float64(0.21729758322704645), 'conflict monitoring': np.float64(0.19307880621020063), 'primary visual cortex (left visual field)': np.float64(0.16039177685466574), 'emotional control': np.float64(0.1422912198739578), 'language processing': np.float64(0.11885432259449821), 'primary visual cortex (right visual field)': np.float64(0.10981409373406031), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.5644547465703036,0.9226456315109921
2,"[0.10119047619047619, 0.3356009070294784, 0.23983193277310924, 0.35259740259740263, 0.3168181818181818]","[np.float64(0.7451062296263535), np.float64(0.6661424803839664), np.float64(0.6668311403508771), np.float64(0.6777519779841762), np.float64(0.7088310543515652)]","{np.str_('S1'): np.float64(0.1875), np.str_('S2'): np.float64(0.8947368421052632), np.str_('S3'): np.float64(0.42857142857142855), np.str_('S4'): np.float64(0.0), np.str_('S5'): np.float64(0.0), np.str_('S6'): np.float64(0.375), np.str_('S7'): np.float64(0.1)}","{np.str_('S1'): np.int64(16), np.str_('S2'): np.int64(19), np.str_('S3'): np.int64(14), np.str_('S4'): np.int64(10), np.str_('S5'): np.int64(7), np.str_('S6'): np.int64(16), np.str_('S7'): np.int64(20)}","{'motor planning': np.float64(0.9901341047264622), 'language': np.float64(0.9360085723406227), 'language processing': np.float64(0.7760888700796326), 'conflict monitoring': np.float64(0.6885473597733063), 'auditory processing': np.float64(0.6306805683453987), 'working memory': np.float64(0.4407546922459405), 'object recognition': np.float64(0.42551234549816064), 'decision making': np.float64(0.3655625574338045), 'emotional control': np.float64(0.35947960752111324), 'speech and language integration': np.float64(0.3435999146102944), 'sound localization': np.float64(0.29014719576709364), 'high-level visual processing': np.float64(0.27305677531869743), 'auditory-motor integration': np.float64(0.2637038758174634), 'speech perception': np.float64(0.2549155385722041), 'visual integration': np.float64(0.25434970082210784), 'visual spatial processing': np.float64(0.24323162475781096), 'primary visual cortex (left visual field)': np.float64(0.22711795964831713), 'primary auditory processing': np.float64(0.21863241190953073), 'primary visual cortex (right visual field)': np.float64(0.21861678983588237), 'executive function': np.float64(0.17283036122650505), 'attention': np.float64(0.1422641715403142), 'sensorimotor integration': np.float64(0.09428433989984451), 'visual attention': np.float64(0.08859339330746209), 'spatial processing': np.float64(0.070588668708948), 'motor cortex - movement of left limbs': np.float64(0.036920327759798414), 'tactile processing': np.float64(0.00921197277589538), 'spatial awareness': np.float64(0.004054601298416499), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'executive control': np.float64(0.0), 'emotion': np.float64(0.0), 'cognitive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'auditory response': np.float64(0.0), 'auditory association': np.float64(0.0)}",0.26920778008172963,0.6929325765393877
3,"[0.8219610286620594, 0.809584499937961, 0.7876710014528787, 0.7609106367510671, 0.7385215361390478]","[np.float64(0.9606383471083332), np.float64(0.9032490643635392), np.float64(0.932320773281287), np.float64(0.9169410264937368), np.float64(0.9547952924657283)]","{np.str_('S3'): np.float64(0.7566539923954373), np.str_('S5'): np.float64(0.9506172839506173), np.str_('S6'): np.float64(0.6341463414634146), np.str_('S7'): np.float64(0.8333333333333334)}","{np.str_('S3'): np.int64(263), np.str_('S5'): np.int64(81), np.str_('S6'): np.int64(123), np.str_('S7'): np.int64(18)}","{'language': np.float64(2.101810599706477), 'language processing': np.float64(1.6363396267556076), 'emotional control': np.float64(1.5320424526033516), 'working memory': np.float64(1.4356202969278546), 'conflict monitoring': np.float64(1.3340482541351897), 'auditory processing': np.float64(1.2346760813247495), 'motor cortex - movement of left limbs': np.float64(1.172405059170296), 'speech and language integration': np.float64(1.0928628442222046), 'decision making': np.float64(1.0833569276555148), 'spatial processing': np.float64(1.048324317399495), 'motor planning': np.float64(0.9999272373631571), 'sensorimotor integration': np.float64(0.8691012327698718), 'executive function': np.float64(0.8309728987361286), 'auditory association': np.float64(0.7736055998586563), 'speech perception': np.float64(0.7586032892387006), 'attention': np.float64(0.7533344258182625), 'primary auditory processing': np.float64(0.6895012780205408), 'auditory-motor integration': np.float64(0.6868683642525181), 'sound localization': np.float64(0.6606602423391719), 'object recognition': np.float64(0.3749405128079135), 'motor cortex - movement of right limbs': np.float64(0.3058379735878139), 'visual attention': np.float64(0.24465750848367906), 'primary visual cortex (right visual field)': np.float64(0.02418729633875977), 'primary visual cortex (left visual field)': np.float64(0.004485189299232197), 'somatosensory integration': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial awareness': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visual integration': np.float64(0.0), 'visual association area': np.float64(0.0), 'tactile processing': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'emotion': np.float64(0.0), 'high-level visual processing': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.7837297405886028,0.9335889007425248
5,"[0.567022607022607, 0.4658664567755477, 0.36784511784511786, 0.46299956082564775, 0.4905681818181818]","[np.float64(0.8365839674869912), np.float64(0.8168113828533619), np.float64(0.8390156736483573), np.float64(0.8006054038078347), np.float64(0.8607286535303778)]","{np.str_('S2'): np.float64(0.19047619047619047), np.str_('S3'): np.float64(0.9615384615384616), np.str_('S4'): np.float64(0.3333333333333333), np.str_('S5'): np.float64(0.34285714285714286), np.str_('S6'): np.float64(0.16666666666666666), np.str_('S7'): np.float64(0.3076923076923077)}","{np.str_('S2'): np.int64(21), np.str_('S3'): np.int64(52), np.str_('S4'): np.int64(18), np.str_('S5'): np.int64(35), np.str_('S6'): np.int64(12), np.str_('S7'): np.int64(26)}","{'auditory processing': np.float64(1.592614798323552), 'language processing': np.float64(1.2085898269920952), 'speech perception': np.float64(0.7744197796573192), 'language': np.float64(0.5588103065856063), 'motor planning': np.float64(0.5390929010935506), 'decision making': np.float64(0.537079380950025), 'sound localization': np.float64(0.5246061448297697), 'primary auditory processing': np.float64(0.43793773791171947), 'auditory-motor integration': np.float64(0.4064002828903322), 'spatial processing': np.float64(0.3674337366751851), 'object recognition': np.float64(0.32748331804801895), 'primary visual cortex (left visual field)': np.float64(0.295635552489936), 'speech and language integration': np.float64(0.2808968502413987), 'visual attention': np.float64(0.27133524704060635), 'sensorimotor integration': np.float64(0.25202446611663437), 'executive function': np.float64(0.2420559679698688), 'high-level visual processing': np.float64(0.21335204775281957), 'working memory': np.float64(0.19678846898847926), 'attention': np.float64(0.17469502355187186), 'auditory association': np.float64(0.1410811053465286), 'visual spatial processing': np.float64(0.13669342974597534), 'visual integration': np.float64(0.11794748802261544), 'primary visual cortex (right visual field)': np.float64(0.11217174486568514), 'conflict monitoring': np.float64(0.07317734848472579), 'spatial awareness': np.float64(0.06617226560171605), 'tactile processing': np.float64(0.06315341605062856), 'emotional control': np.float64(0.045538118147599276), 'motor cortex - movement of left limbs': np.float64(0.0299996966007995), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.4708603848574205,0.8307490162653846
6,"[0.10050818746470921, 0.06521739130434782, 0.11594202898550723, 0.10802139037433157, 0.08522727272727272]","[np.float64(0.5387127238385819), np.float64(0.5727198430859758), np.float64(0.5128945552286513), np.float64(0.49875770084748416), np.float64(0.5453677643306126)]","{np.str_('S1'): np.float64(0.0), np.str_('S2'): np.float64(0.0), np.str_('S3'): np.float64(0.6363636363636364), np.str_('S4'): np.float64(0.0), np.str_('S5'): np.float64(0.07142857142857142), np.str_('S6'): np.float64(0.23076923076923078), np.str_('S7'): np.float64(0.0)}","{np.str_('S1'): np.int64(10), np.str_('S2'): np.int64(15), np.str_('S3'): np.int64(22), np.str_('S4'): np.int64(23), np.str_('S5'): np.int64(14), np.str_('S6'): np.int64(13), np.str_('S7'): np.int64(16)}","{'sound localization': np.float64(0.798516302133583), 'auditory processing': np.float64(0.7291347943754193), 'speech perception': np.float64(0.7226278967359141), 'spatial processing': np.float64(0.42250336839160074), 'auditory association': np.float64(0.35212254979946733), 'auditory-motor integration': np.float64(0.2719690714544007), 'speech and language integration': np.float64(0.24264355276190994), 'primary auditory processing': np.float64(0.2087968250318498), 'visual attention': np.float64(0.20771886621667585), 'motor cortex - movement of left limbs': np.float64(0.18702114230027164), 'language': np.float64(0.17506278722247043), 'language processing': np.float64(0.17285563433828685), 'object recognition': np.float64(0.11568559233713659), 'sensorimotor integration': np.float64(0.10329188230849994), 'decision making': np.float64(0.06775965700027956), 'motor planning': np.float64(0.05826237868285332), 'working memory': np.float64(0.027103862800111827), 'attention': np.float64(0.024288816167531764), 'executive function': np.float64(0.024288816167531764), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visual integration': np.float64(0.0), 'visual association area': np.float64(0.0), 'tactile processing': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial awareness': np.float64(0.0), 'spatial attention': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'primary visual cortex (right visual field)': np.float64(0.0), 'primary visual cortex (left visual field)': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'high-level visual processing': np.float64(0.0), 'conflict monitoring': np.float64(0.0), 'emotion': np.float64(0.0), 'emotional control': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.0949832541712337,0.5336905174662612
7,"[0.7771134756837821, 0.7166817428011458, 0.7548518896833504, 0.6733415016977384, 0.7691294213033344]","[np.float64(0.9659272742055801), np.float64(0.924584466482397), np.float64(0.9666091583854741), np.float64(0.9158541306518427), np.float64(0.9458024818318936)]","{np.str_('S3'): np.float64(0.2), np.str_('S5'): np.float64(0.7943548387096774), np.str_('S6'): np.float64(0.8387096774193549), np.str_('S7'): np.float64(0.22916666666666666)}","{np.str_('S3'): np.int64(5), np.str_('S5'): np.int64(248), np.str_('S6'): np.int64(31), np.str_('S7'): np.int64(48)}","{'executive function': np.float64(1.9617088065531185), 'attention': np.float64(1.9260285411892195), 'speech perception': np.float64(1.8774287463061226), 'decision making': np.float64(1.8705260309467138), 'language': np.float64(1.7201543045602523), 'sound localization': np.float64(1.6537608936474433), 'motor planning': np.float64(1.0103384400259134), 'auditory processing': np.float64(0.9986848996147483), 'high-level visual processing': np.float64(0.8907260770676141), 'auditory-motor integration': np.float64(0.7544135021798057), 'language processing': np.float64(0.7065896092552411), 'primary visual cortex (right visual field)': np.float64(0.595891091687803), 'working memory': np.float64(0.5691627614909647), 'auditory association': np.float64(0.56051389414047), 'primary auditory processing': np.float64(0.5138089398597597), 'spatial processing': np.float64(0.478561560664176), 'conflict monitoring': np.float64(0.47037540359726276), 'visual attention': np.float64(0.32451113645584656), 'primary visual cortex (left visual field)': np.float64(0.2568600352725636), 'emotional control': np.float64(0.24473118047657758), 'object recognition': np.float64(0.21476667441856412), 'motor cortex - movement of left limbs': np.float64(0.19372895663959425), 'sensorimotor integration': np.float64(0.19115102099600376), 'speech and language integration': np.float64(0.13772687829708402), 'visual integration': np.float64(0.08426150031466247), 'visuospatial attention': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'spatial awareness': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visual association area': np.float64(0.0), 'tactile processing': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.7382236062338703,0.9437555023114376
8,"[0.2930429522081065, 0.22619014379449948, 0.3239587998208688, 0.27783251231527095, 0.3208128078817734]","[np.float64(0.7425464190981433), np.float64(0.7296689262206504), np.float64(0.7209244523037626), np.float64(0.6924403183023872), np.float64(0.6755702917771884)]","{np.str_('S1'): np.float64(0.3076923076923077), np.str_('S2'): np.float64(0.0), np.str_('S3'): np.float64(0.6444444444444445), np.str_('S4'): np.float64(0.045454545454545456), np.str_('S5'): np.float64(0.1111111111111111), np.str_('S6'): np.float64(0.5333333333333333), np.str_('S7'): np.float64(0.0)}","{np.str_('S1'): np.int64(13), np.str_('S2'): np.int64(16), np.str_('S3'): np.int64(45), np.str_('S4'): np.int64(22), np.str_('S5'): np.int64(18), np.str_('S6'): np.int64(15), np.str_('S7'): np.int64(16)}","{'object recognition': np.float64(0.8803343611648703), 'decision making': np.float64(0.7787526548386258), 'speech and language integration': np.float64(0.5479693830514558), 'high-level visual processing': np.float64(0.515440568664213), 'language processing': np.float64(0.4569557205754536), 'visual integration': np.float64(0.43565355799539035), 'auditory processing': np.float64(0.4348748148266911), 'spatial processing': np.float64(0.3856720949584431), 'visual spatial processing': np.float64(0.3777035182859323), 'auditory-motor integration': np.float64(0.3387554079771284), 'executive function': np.float64(0.2982650247694878), 'attention': np.float64(0.2909429171263325), 'language': np.float64(0.29037687353972474), 'working memory': np.float64(0.2902237770654558), 'visual attention': np.float64(0.27070700432220524), 'sound localization': np.float64(0.255229978967244), 'motor planning': np.float64(0.25245553542751853), 'speech perception': np.float64(0.18967529683626538), 'motor cortex - movement of left limbs': np.float64(0.17452375761498004), 'auditory association': np.float64(0.17220245828941344), 'spatial awareness': np.float64(0.1313282897530083), 'sensorimotor integration': np.float64(0.10293468835207609), 'primary visual cortex (left visual field)': np.float64(0.09138512936872155), 'conflict monitoring': np.float64(0.05945057586033372), 'emotional control': np.float64(0.03699591767309183), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'spatial attention': np.float64(0.0), 'tactile processing': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'primary visual cortex (right visual field)': np.float64(0.0), 'primary auditory processing': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.2883674432041038,0.7122300815404264
9,"[0.0884643316015865, 0.14892121090951502, 0.3378147713659696, 0.25870940968980183, 0.21331050938894072]","[np.float64(0.5703182400459869), np.float64(0.629673099325042), np.float64(0.6440672788113431), np.float64(0.6230531429517708), np.float64(0.6770950156652695)]","{np.str_('S1'): np.float64(0.13636363636363635), np.str_('S2'): np.float64(0.3170731707317073), np.str_('S3'): np.float64(0.2), np.str_('S4'): np.float64(0.14705882352941177), np.str_('S5'): np.float64(0.0), np.str_('S6'): np.float64(0.15384615384615385), np.str_('S7'): np.float64(0.9696969696969697)}","{np.str_('S1'): np.int64(44), np.str_('S2'): np.int64(41), np.str_('S3'): np.int64(45), np.str_('S4'): np.int64(34), np.str_('S5'): np.int64(34), np.str_('S6'): np.int64(39), np.str_('S7'): np.int64(33)}","{'object recognition': np.float64(1.0795283876085577), 'motor planning': np.float64(0.9170814134406624), 'visual spatial processing': np.float64(0.8584858911408547), 'auditory-motor integration': np.float64(0.7151261284545766), 'decision making': np.float64(0.7139150563581561), 'sensorimotor integration': np.float64(0.6921381675753054), 'sound localization': np.float64(0.6398840519597977), 'high-level visual processing': np.float64(0.622363802141521), 'speech and language integration': np.float64(0.5974863516011975), 'language': np.float64(0.5955111565823785), 'primary visual cortex (right visual field)': np.float64(0.584438764669154), 'visual integration': np.float64(0.543757853828072), 'spatial processing': np.float64(0.5137657056170752), 'conflict monitoring': np.float64(0.4807366830073182), 'attention': np.float64(0.4698569436826188), 'language processing': np.float64(0.4660382307317585), 'executive function': np.float64(0.4647645660903955), 'auditory association': np.float64(0.4541199425086855), 'working memory': np.float64(0.4440805075623812), 'auditory processing': np.float64(0.4140106923496667), 'speech perception': np.float64(0.410624676206104), 'emotional control': np.float64(0.3649598687729889), 'spatial awareness': np.float64(0.32522216744617), 'visual attention': np.float64(0.3065800676416539), 'primary visual cortex (left visual field)': np.float64(0.29984673721506766), 'primary auditory processing': np.float64(0.2223740495110634), 'tactile processing': np.float64(0.16238375672403987), 'motor cortex - movement of right limbs': np.float64(0.13861642768321492), 'motor cortex - movement of left limbs': np.float64(0.12819517498913682), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.20944404659116275,0.6288413553598825
10,"[0.6380329909741673, 0.6253209305112418, 0.6768065268065269, 0.7307507307507307, 0.6162621692033456]","[np.float64(0.8649698340874812), np.float64(0.9244343891402714), np.float64(0.84930990062569), np.float64(0.9243299954733711), np.float64(0.8344618599791014)]","{np.str_('S2'): np.float64(0.7631578947368421), np.str_('S3'): np.float64(0.2926829268292683), np.str_('S4'): np.float64(0.9473684210526315), np.str_('S7'): np.float64(0.7536231884057971)}","{np.str_('S2'): np.int64(38), np.str_('S3'): np.int64(41), np.str_('S4'): np.int64(19), np.str_('S7'): np.int64(69)}","{'language': np.float64(1.8492156406162772), 'motor cortex - movement of left limbs': np.float64(0.9927434079463687), 'motor planning': np.float64(0.9900729015677984), 'auditory-motor integration': np.float64(0.8676561191103266), 'auditory processing': np.float64(0.8429528070668422), 'auditory association': np.float64(0.7658770774304221), 'language processing': np.float64(0.7319738791868179), 'decision making': np.float64(0.6176171019196127), 'attention': np.float64(0.6138997896520564), 'executive function': np.float64(0.6138997896520564), 'working memory': np.float64(0.5172173135617621), 'speech perception': np.float64(0.49764552903534814), 'sound localization': np.float64(0.3135066742635962), 'emotional control': np.float64(0.297512550528325), 'visual spatial processing': np.float64(0.2374955808050944), 'conflict monitoring': np.float64(0.2265790950427166), 'primary auditory processing': np.float64(0.16716946723657003), 'spatial processing': np.float64(0.12276667474791969), 'object recognition': np.float64(0.0883818641648104), 'visual attention': np.float64(0.0855228569609372), 'sensorimotor integration': np.float64(0.05479756733792299), 'visual integration': np.float64(0.0397145692313956), 'high-level visual processing': np.float64(0.036768147011125143), 'speech and language integration': np.float64(0.03042083992337126), 'tactile processing': np.float64(0.012787523329835385), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'spatial awareness': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'primary visual cortex (left visual field)': np.float64(0.0), 'primary visual cortex (right visual field)': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.6574346696492025,0.879501195861183
11,"[0.28217636022514075, 0.14634146341463414, 0.23365853658536587, 0.16416666666666666, 0.2616550116550117]","[np.float64(0.7332156841482523), np.float64(0.6941882667851104), np.float64(0.6561028402491818), np.float64(0.6768992337926161), np.float64(0.7333227230286055)]","{np.str_('S1'): np.float64(0.9411764705882353), np.str_('S2'): np.float64(0.21739130434782608), np.str_('S3'): np.float64(0.35714285714285715), np.str_('S4'): np.float64(0.07692307692307693), np.str_('S5'): np.float64(0.0967741935483871), np.str_('S6'): np.float64(0.034482758620689655), np.str_('S7'): np.float64(0.10526315789473684)}","{np.str_('S1'): np.int64(34), np.str_('S2'): np.int64(23), np.str_('S3'): np.int64(28), np.str_('S4'): np.int64(39), np.str_('S5'): np.int64(31), np.str_('S6'): np.int64(29), np.str_('S7'): np.int64(19)}","{'motor planning': np.float64(1.038615197716275), 'object recognition': np.float64(0.9281688104450455), 'primary auditory processing': np.float64(0.9102423938522574), 'auditory-motor integration': np.float64(0.8825131569624997), 'decision making': np.float64(0.7675135261199356), 'language': np.float64(0.741886409412203), 'sensorimotor integration': np.float64(0.6773155655300924), 'high-level visual processing': np.float64(0.6184332049835523), 'auditory processing': np.float64(0.5825030727031818), 'speech perception': np.float64(0.5671097116471591), 'visual attention': np.float64(0.5136178228260238), 'sound localization': np.float64(0.5025061077531777), 'visual spatial processing': np.float64(0.4958554216910115), 'spatial processing': np.float64(0.4621192916420254), 'spatial awareness': np.float64(0.44856664572011173), 'visual integration': np.float64(0.44314016891870456), 'tactile processing': np.float64(0.40399897021382153), 'language processing': np.float64(0.3755751304428386), 'speech and language integration': np.float64(0.35737684475631154), 'primary visual cortex (right visual field)': np.float64(0.35057277157604744), 'primary visual cortex (left visual field)': np.float64(0.3452698025292483), 'conflict monitoring': np.float64(0.3325766803536392), 'auditory association': np.float64(0.31516380217265877), 'attention': np.float64(0.2191913435293374), 'motor cortex - movement of right limbs': np.float64(0.17156312264429147), 'motor cortex - movement of left limbs': np.float64(0.15942068284569538), 'executive function': np.float64(0.15453420486193647), 'working memory': np.float64(0.13367911646970196), 'emotional control': np.float64(0.051609853980607905), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.21759960770936382,0.6987457496007531
12,"[0.32176196589033557, 0.331496957521913, 0.2895188208957857, 0.24239266332289588, 0.3326562478409264]","[np.float64(0.744901666593119), np.float64(0.7595463612372501), np.float64(0.7241324290826647), np.float64(0.6668850995392887), np.float64(0.781838668024845)]","{np.str_('S1'): np.float64(0.10256410256410256), np.str_('S2'): np.float64(0.2222222222222222), np.str_('S3'): np.float64(0.5294117647058824), np.str_('S4'): np.float64(0.8461538461538461), np.str_('S5'): np.float64(0.14925373134328357), np.str_('S6'): np.float64(0.2375), np.str_('S7'): np.float64(0.3333333333333333)}","{np.str_('S1'): np.int64(78), np.str_('S2'): np.int64(81), np.str_('S3'): np.int64(34), np.str_('S4'): np.int64(78), np.str_('S5'): np.int64(67), np.str_('S6'): np.int64(80), np.str_('S7'): np.int64(78)}","{'sound localization': np.float64(3.3757908511191514), 'language processing': np.float64(1.2215027337876703), 'motor planning': np.float64(0.9851241080746537), 'auditory processing': np.float64(0.9182391035575928), 'object recognition': np.float64(0.5441062684032258), 'visual spatial processing': np.float64(0.5201265727335338), 'speech perception': np.float64(0.5164014527537623), 'executive function': np.float64(0.4974815040041346), 'spatial processing': np.float64(0.47042643073141205), 'attention': np.float64(0.43440029404050895), 'decision making': np.float64(0.3816519167115321), 'primary auditory processing': np.float64(0.3495052188502929), 'auditory association': np.float64(0.3314567777520284), 'sensorimotor integration': np.float64(0.28396073229808294), 'high-level visual processing': np.float64(0.273898537340908), 'visual integration': np.float64(0.2731380508444311), 'speech and language integration': np.float64(0.2685788928882151), 'auditory-motor integration': np.float64(0.2505166366796249), 'tactile processing': np.float64(0.1972303633156388), 'visual attention': np.float64(0.19702325401960935), 'language': np.float64(0.1946983976578387), 'motor cortex - movement of right limbs': np.float64(0.1686771099441999), 'primary visual cortex (right visual field)': np.float64(0.1579683396598679), 'motor cortex - movement of left limbs': np.float64(0.14384185448504272), 'primary visual cortex (left visual field)': np.float64(0.13852337929020295), 'conflict monitoring': np.float64(0.09045412818104352), 'spatial awareness': np.float64(0.08586228397811915), 'working memory': np.float64(0.06497943383142614), 'emotional control': np.float64(0.04591345442367428), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.30356533109437134,0.7354608448954334
13,"[0.5051587301587303, 0.49637188208616784, 0.21153846153846154, 0.475, 0.4107843137254902]","[np.float64(0.7223370927318296), np.float64(0.781204850361197), np.float64(0.6841008771929824), np.float64(0.6555122180451127), np.float64(0.6545582706766917)]","{np.str_('S1'): np.float64(0.39285714285714285), np.str_('S2'): np.float64(0.8181818181818182), np.str_('S3'): np.float64(0.1), np.str_('S4'): np.float64(0.5), np.str_('S5'): np.float64(0.75), np.str_('S6'): np.float64(0.42857142857142855), np.str_('S7'): np.float64(0.3333333333333333)}","{np.str_('S1'): np.int64(28), np.str_('S2'): np.int64(11), np.str_('S3'): np.int64(20), np.str_('S4'): np.int64(6), np.str_('S5'): np.int64(24), np.str_('S6'): np.int64(7), np.str_('S7'): np.int64(6)}","{'sound localization': np.float64(1.0687369363895967), 'primary auditory processing': np.float64(1.0540349558988533), 'working memory': np.float64(0.6886933072544164), 'conflict monitoring': np.float64(0.6575130988297699), 'emotional control': np.float64(0.5789939105260429), 'language': np.float64(0.5619688614738129), 'auditory processing': np.float64(0.4053626876726166), 'language processing': np.float64(0.3991134129155102), 'decision making': np.float64(0.3979594668598413), 'speech and language integration': np.float64(0.35577888727717355), 'attention': np.float64(0.28698840136883835), 'executive function': np.float64(0.285464858655116), 'motor planning': np.float64(0.28226811377762523), 'auditory association': np.float64(0.26296887935876445), 'speech perception': np.float64(0.2479007329823844), 'auditory-motor integration': np.float64(0.2115656693371175), 'high-level visual processing': np.float64(0.18492032265204017), 'tactile processing': np.float64(0.16266269989136603), 'sensorimotor integration': np.float64(0.15077664672752333), 'object recognition': np.float64(0.13060175369092294), 'motor cortex - movement of left limbs': np.float64(0.10511982972505139), 'spatial processing': np.float64(0.0923596887516525), 'visual attention': np.float64(0.08694804273315784), 'primary visual cortex (left visual field)': np.float64(0.07563274986468987), 'motor cortex - movement of right limbs': np.float64(0.05641024578212376), 'visual integration': np.float64(0.03496791255822145), 'primary visual cortex (right visual field)': np.float64(0.012980815527948074), 'spatial awareness': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.4197706775017699,0.6995426618015627
14,"[0.6373456790123457, 0.5869281045751635, 0.48888888888888893, 0.5213196389666979, 0.45601851851851855]","[np.float64(0.9279003267973857), np.float64(0.836922268907563), np.float64(0.8348214285714285), np.float64(0.8880174291938998), np.float64(0.9169759570494863)]","{np.str_('S1'): np.float64(0.6111111111111112), np.str_('S2'): np.float64(0.5714285714285714), np.str_('S3'): np.float64(0.0), np.str_('S6'): np.float64(0.6363636363636364), np.str_('S7'): np.float64(0.0)}","{np.str_('S1'): np.int64(54), np.str_('S2'): np.int64(7), np.str_('S3'): np.int64(6), np.str_('S6'): np.int64(11), np.str_('S7'): np.int64(12)}","{'language': np.float64(0.9430417143336051), 'primary auditory processing': np.float64(0.8167005094276194), 'auditory association': np.float64(0.7335182701914399), 'sound localization': np.float64(0.7212179214405275), 'motor planning': np.float64(0.6354620445303433), 'executive function': np.float64(0.6252337809415607), 'attention': np.float64(0.597587360906795), 'primary visual cortex (left visual field)': np.float64(0.5938006754733662), 'spatial processing': np.float64(0.5878809514424264), 'speech and language integration': np.float64(0.5566258969141809), 'motor cortex - movement of right limbs': np.float64(0.5463162587242365), 'auditory processing': np.float64(0.49388229919034066), 'object recognition': np.float64(0.45470341503876116), 'spatial awareness': np.float64(0.3969004927501133), 'visual spatial processing': np.float64(0.38385236204784284), 'high-level visual processing': np.float64(0.3503670094094842), 'primary visual cortex (right visual field)': np.float64(0.3222821695824), 'visual integration': np.float64(0.3165887863024999), 'visual attention': np.float64(0.28135124614208457), 'language processing': np.float64(0.26465277166613255), 'speech perception': np.float64(0.243158620769794), 'auditory-motor integration': np.float64(0.21598332543850537), 'conflict monitoring': np.float64(0.1904894444165701), 'sensorimotor integration': np.float64(0.18962854340525148), 'tactile processing': np.float64(0.09917115227376669), 'working memory': np.float64(0.09358554832341123), 'motor cortex - movement of left limbs': np.float64(0.08854904500320442), 'emotional control': np.float64(0.07327319005337843), 'decision making': np.float64(0.050782887680332456), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.5381001659923229,0.8809274821039527
16,"[0.19247589145548327, 0.3815355089404183, 0.33767660910518055, 0.30265244959122506, 0.24680536477411477]","[np.float64(0.6191950506080711), np.float64(0.7158955086076518), np.float64(0.7174032237015118), np.float64(0.7076923548878619), np.float64(0.6700564583584202)]","{np.str_('S1'): np.float64(0.0), np.str_('S2'): np.float64(0.5813953488372093), np.str_('S3'): np.float64(0.6428571428571429), np.str_('S4'): np.float64(0.6), np.str_('S5'): np.float64(0.1951219512195122), np.str_('S6'): np.float64(0.08333333333333333), np.str_('S7'): np.float64(0.38461538461538464)}","{np.str_('S1'): np.int64(38), np.str_('S2'): np.int64(43), np.str_('S3'): np.int64(28), np.str_('S4'): np.int64(45), np.str_('S5'): np.int64(41), np.str_('S6'): np.int64(36), np.str_('S7'): np.int64(13)}","{'primary auditory processing': np.float64(1.334553830277092), 'motor planning': np.float64(1.0941071075935458), 'auditory processing': np.float64(1.0272168669685584), 'speech perception': np.float64(0.7414766715372492), 'sound localization': np.float64(0.7292226527988592), 'decision making': np.float64(0.7195495293788458), 'working memory': np.float64(0.649656055310416), 'conflict monitoring': np.float64(0.6018453785591459), 'attention': np.float64(0.585550904903123), 'emotional control': np.float64(0.5705171442593583), 'executive function': np.float64(0.44822547003857804), 'sensorimotor integration': np.float64(0.4228467705386611), 'motor cortex - movement of right limbs': np.float64(0.387155784677037), 'language': np.float64(0.33732141165939616), 'auditory-motor integration': np.float64(0.3352512403382844), 'speech and language integration': np.float64(0.32121243713016745), 'high-level visual processing': np.float64(0.31714821733729237), 'object recognition': np.float64(0.2699624312684591), 'language processing': np.float64(0.2427885227096653), 'spatial processing': np.float64(0.23540482292952727), 'visual integration': np.float64(0.20143537573386933), 'visual attention': np.float64(0.18350040339982496), 'tactile processing': np.float64(0.1830788678627705), 'motor cortex - movement of left limbs': np.float64(0.1417106174327235), 'auditory association': np.float64(0.11225673847243282), 'spatial awareness': np.float64(0.10306517547927382), 'primary visual cortex (right visual field)': np.float64(0.056358292871205716), 'spatial attention': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'primary visual cortex (left visual field)': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.2922291647732844,0.6860485192327034
17,"[0.5888888888888889, 0.6022408963585434, 0.40392156862745093, 0.6823529411764706, 0.5588235294117647]","[np.float64(0.6770833333333334), np.float64(0.7213235294117648), np.float64(0.803921568627451), np.float64(0.8468137254901961), np.float64(0.5341386554621849)]","{np.str_('S1'): np.float64(1.0), np.str_('S2'): np.float64(0.0), np.str_('S4'): np.float64(0.6792452830188679), np.str_('S5'): np.float64(0.5555555555555556), np.str_('S7'): np.float64(0.09090909090909091)}","{np.str_('S1'): np.int64(7), np.str_('S2'): np.int64(6), np.str_('S4'): np.int64(53), np.str_('S5'): np.int64(9), np.str_('S7'): np.int64(11)}","{'conflict monitoring': np.float64(0.9014398373670011), 'sound localization': np.float64(0.8318166568496531), 'auditory association': np.float64(0.8289613420260281), 'spatial processing': np.float64(0.6968477677662042), 'sensorimotor integration': np.float64(0.5853066643323059), 'speech and language integration': np.float64(0.5634498421741091), 'auditory processing': np.float64(0.5238383506280117), 'language processing': np.float64(0.49967058482869875), 'visual attention': np.float64(0.4824330699919875), 'spatial awareness': np.float64(0.3209711721812495), 'emotional control': np.float64(0.2985055997984805), 'working memory': np.float64(0.2984336292385385), 'speech perception': np.float64(0.25382708778630325), 'language': np.float64(0.25368997887229544), 'motor cortex - movement of right limbs': np.float64(0.2078502700206256), 'motor planning': np.float64(0.1986913477070909), 'auditory-motor integration': np.float64(0.17414962388154248), 'executive function': np.float64(0.1701768682686213), 'attention': np.float64(0.1701768682686213), 'decision making': np.float64(0.14568353043644408), 'primary visual cortex (right visual field)': np.float64(0.1358321936389725), 'sensorimotor function': np.float64(0.0), 'visual integration': np.float64(0.0), 'visual association area': np.float64(0.0), 'tactile processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'object recognition': np.float64(0.0), 'primary auditory processing': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor cortex - movement of left limbs': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'primary visual cortex (left visual field)': np.float64(0.0), 'emotion': np.float64(0.0), 'high-level visual processing': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.5672455648926237,0.716656162464986
18,"[0.2764287324125052, 0.22542497856303623, 0.21053273974166467, 0.24738396067821986, 0.22331359572738882]","[np.float64(0.7208159884021953), np.float64(0.7117841979910946), np.float64(0.7286631459045252), np.float64(0.7062131096613856), np.float64(0.7370570570570572)]","{np.str_('S1'): np.float64(0.16393442622950818), np.str_('S2'): np.float64(0.36507936507936506), np.str_('S3'): np.float64(0.703125), np.str_('S4'): np.float64(0.4262295081967213), np.str_('S5'): np.float64(0.0), np.str_('S6'): np.float64(0.23809523809523808), np.str_('S7'): np.float64(0.21311475409836064)}","{np.str_('S1'): np.int64(61), np.str_('S2'): np.int64(63), np.str_('S3'): np.int64(64), np.str_('S4'): np.int64(61), np.str_('S5'): np.int64(62), np.str_('S6'): np.int64(63), np.str_('S7'): np.int64(61)}","{'speech perception': np.float64(2.3898420813979535), 'auditory processing': np.float64(1.9725521696232464), 'primary visual cortex (right visual field)': np.float64(1.4031650261697), 'executive function': np.float64(1.36357767870354), 'attention': np.float64(1.3510663192510104), 'language processing': np.float64(0.49602508650898214), 'motor planning': np.float64(0.36163764406665067), 'object recognition': np.float64(0.3226599775237043), 'spatial processing': np.float64(0.28113004346741716), 'sensorimotor integration': np.float64(0.26872366717656937), 'language': np.float64(0.2497031144917633), 'sound localization': np.float64(0.2423327169380428), 'speech and language integration': np.float64(0.2197799766851403), 'visual attention': np.float64(0.21599799058454217), 'decision making': np.float64(0.2069781006921883), 'high-level visual processing': np.float64(0.18223789937855137), 'primary auditory processing': np.float64(0.16025901831605718), 'visual spatial processing': np.float64(0.14603504216462257), 'motor cortex - movement of right limbs': np.float64(0.13481561927478017), 'auditory-motor integration': np.float64(0.1207990498141154), 'visual integration': np.float64(0.11487942453483761), 'conflict monitoring': np.float64(0.10806799263172301), 'tactile processing': np.float64(0.09796308850566768), 'spatial awareness': np.float64(0.08650119687667068), 'auditory association': np.float64(0.08460707400306329), 'working memory': np.float64(0.0705971982230071), 'motor cortex - movement of left limbs': np.float64(0.06896021596824965), 'primary visual cortex (left visual field)': np.float64(0.06019037478197576), 'emotional control': np.float64(0.04756657986886635), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.23661680142456296,0.7209066998032515
19,"[0.6720942215679058, 0.665, 0.8352380952380953, 0.7866666666666666, 0.5533157894736843]","[np.float64(0.9120971867007672), np.float64(0.9316265984654731), np.float64(0.9513401534526855), np.float64(0.9180542264752792), np.float64(0.8901329080276449)]","{np.str_('S1'): np.float64(0.25), np.str_('S3'): np.float64(0.8541666666666666), np.str_('S5'): np.float64(0.47368421052631576), np.str_('S6'): np.float64(0.9259259259259259)}","{np.str_('S1'): np.int64(12), np.str_('S3'): np.int64(48), np.str_('S5'): np.int64(38), np.str_('S6'): np.int64(27)}","{'object recognition': np.float64(1.0180600283792451), 'visual spatial processing': np.float64(0.7076098783567241), 'auditory processing': np.float64(0.687662256003132), 'spatial processing': np.float64(0.6766666535217424), 'speech perception': np.float64(0.5130835341597955), 'visual attention': np.float64(0.4816814967464775), 'auditory association': np.float64(0.4724406480575329), 'high-level visual processing': np.float64(0.4685235185296002), 'visual integration': np.float64(0.4543925862934193), 'primary auditory processing': np.float64(0.450095726114895), 'motor planning': np.float64(0.44535611417602516), 'speech and language integration': np.float64(0.4196860915401783), 'language processing': np.float64(0.35833325589127796), 'executive function': np.float64(0.28298466554872664), 'attention': np.float64(0.2621498998405976), 'motor cortex - movement of left limbs': np.float64(0.23950915419898436), 'primary visual cortex (left visual field)': np.float64(0.22492001792389052), 'sensorimotor integration': np.float64(0.22465944733064658), 'primary visual cortex (right visual field)': np.float64(0.2025375623066766), 'language': np.float64(0.16347201537446854), 'motor cortex - movement of right limbs': np.float64(0.13353096155851946), 'auditory-motor integration': np.float64(0.11909075316741197), 'conflict monitoring': np.float64(0.11397267894061415), 'spatial awareness': np.float64(0.11392080609118782), 'tactile processing': np.float64(0.08544060456839087), 'decision making': np.float64(0.04945703381220973), 'working memory': np.float64(0.004807513194525787), 'sound localization': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'spatial attention': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'emotional control': np.float64(0.0), 'executive control': np.float64(0.0), 'emotion': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.7024629545892703,0.9206502146243698
20,"[0.3373531230674088, 0.40890515380311304, 0.3162193448808095, 0.30954356101414926, 0.4078282828282828]","[np.float64(0.816147625921427), np.float64(0.8351092564081787), np.float64(0.7528578843804163), np.float64(0.8093261658668635), np.float64(0.7655766063830839)]","{np.str_('S1'): np.float64(0.9285714285714286), np.str_('S2'): np.float64(0.13636363636363635), np.str_('S3'): np.float64(0.20512820512820512), np.str_('S4'): np.float64(0.3088235294117647), np.str_('S5'): np.float64(0.2413793103448276), np.str_('S6'): np.float64(0.8837209302325582), np.str_('S7'): np.float64(0.07142857142857142)}","{np.str_('S1'): np.int64(14), np.str_('S2'): np.int64(22), np.str_('S3'): np.int64(39), np.str_('S4'): np.int64(68), np.str_('S5'): np.int64(29), np.str_('S6'): np.int64(43), np.str_('S7'): np.int64(28)}","{'language processing': np.float64(1.2194863719238593), 'spatial processing': np.float64(1.211208127668127), 'sound localization': np.float64(1.1811882112676788), 'motor planning': np.float64(1.0747256167462835), 'auditory processing': np.float64(1.0222382580780789), 'object recognition': np.float64(0.7803524793683949), 'speech and language integration': np.float64(0.7380107619330657), 'sensorimotor integration': np.float64(0.7300308812572157), 'high-level visual processing': np.float64(0.7279377610426665), 'decision making': np.float64(0.583848245321403), 'primary visual cortex (left visual field)': np.float64(0.5812323522656347), 'speech perception': np.float64(0.5507138465509926), 'executive function': np.float64(0.49134034258444903), 'motor cortex - movement of left limbs': np.float64(0.4761896803065388), 'working memory': np.float64(0.46078278134705886), 'visual integration': np.float64(0.43828827586801966), 'visual attention': np.float64(0.4193796520111118), 'conflict monitoring': np.float64(0.403008531523058), 'primary auditory processing': np.float64(0.39930117894780537), 'attention': np.float64(0.3882234075482135), 'primary visual cortex (right visual field)': np.float64(0.37344884831568975), 'visual spatial processing': np.float64(0.37213772065572864), 'auditory-motor integration': np.float64(0.3716589133942323), 'language': np.float64(0.3529583267044017), 'emotional control': np.float64(0.33507738875441007), 'tactile processing': np.float64(0.2897691285451455), 'spatial awareness': np.float64(0.26889066632247993), 'motor cortex - movement of right limbs': np.float64(0.2583377509810023), 'auditory association': np.float64(0.1724245937040505), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.35596989311875266,0.7958035077919938
21,"[0.15209452201933404, 0.10458333333333332, 0.11159620098039215, 0.1480392156862745, 0.1581196581196581]","[np.float64(0.5846279550226919), np.float64(0.5514880476981542), np.float64(0.5888605025833287), np.float64(0.5712075265064395), np.float64(0.6066447721332939)]","{np.str_('S1'): np.float64(0.24561403508771928), np.str_('S2'): np.float64(0.3333333333333333), np.str_('S3'): np.float64(0.0), np.str_('S4'): np.float64(0.15625), np.str_('S5'): np.float64(0.0), np.str_('S6'): np.float64(0.6875)}","{np.str_('S1'): np.int64(57), np.str_('S2'): np.int64(54), np.str_('S3'): np.int64(7), np.str_('S4'): np.int64(64), np.str_('S5'): np.int64(43), np.str_('S6'): np.int64(16)}","{'sound localization': np.float64(1.4897104662513478), 'speech and language integration': np.float64(0.9860214190963101), 'language': np.float64(0.9534320714305612), 'visual spatial processing': np.float64(0.9372379643170692), 'spatial processing': np.float64(0.8195575147307077), 'visual attention': np.float64(0.6512622030899712), 'motor planning': np.float64(0.5616970096693408), 'conflict monitoring': np.float64(0.5136906597835084), 'attention': np.float64(0.4660640271050828), 'executive function': np.float64(0.40984374824585346), 'auditory processing': np.float64(0.3699624317797802), 'speech perception': np.float64(0.2580609692960896), 'decision making': np.float64(0.2546730779459459), 'sensorimotor integration': np.float64(0.23457744508791228), 'primary auditory processing': np.float64(0.22754947414937646), 'auditory association': np.float64(0.1956996455191213), 'high-level visual processing': np.float64(0.1850521693050883), 'tactile processing': np.float64(0.1733462873566808), 'motor cortex - movement of left limbs': np.float64(0.14718144762534502), 'language processing': np.float64(0.13589423413689813), 'auditory-motor integration': np.float64(0.13081345876772688), 'spatial awareness': np.float64(0.1303749962638379), 'working memory': np.float64(0.11270419972096862), 'object recognition': np.float64(0.09264470265925945), 'motor cortex - movement of right limbs': np.float64(0.07851112568830135), 'visual integration': np.float64(0.06793944861679027), 'emotional control': np.float64(0.054191082248935096), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'primary visual cortex (left visual field)': np.float64(0.0), 'primary visual cortex (right visual field)': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.1348865860277984,0.5805657607887816
22,"[0.5079710144927536, 0.46115942028985507, 0.538888888888889, 0.6073593073593073, 0.5030758714969241]","[np.float64(0.7785416666666667), np.float64(0.6454166666666666), np.float64(0.7830813553491572), np.float64(0.7946658926728587), np.float64(0.6550739594083247)]","{np.str_('S1'): np.float64(0.5), np.str_('S2'): np.float64(0.25), np.str_('S3'): np.float64(0.1111111111111111), np.str_('S6'): np.float64(0.86), np.str_('S7'): np.float64(0.0)}","{np.str_('S1'): np.int64(20), np.str_('S2'): np.int64(8), np.str_('S3'): np.int64(9), np.str_('S6'): np.int64(50), np.str_('S7'): np.int64(12)}","{'auditory association': np.float64(1.4029838410064894), 'spatial processing': np.float64(0.9417448420115526), 'motor planning': np.float64(0.899088971540858), 'sound localization': np.float64(0.8431924875440643), 'speech and language integration': np.float64(0.7938448023503327), 'primary auditory processing': np.float64(0.4782022709138218), 'language processing': np.float64(0.4037394304343563), 'decision making': np.float64(0.39298931385959596), 'auditory processing': np.float64(0.3909184709954914), 'speech perception': np.float64(0.3899374402773295), 'language': np.float64(0.24444749545769162), 'auditory-motor integration': np.float64(0.19397500155311215), 'motor cortex - movement of left limbs': np.float64(0.1593469368082297), 'working memory': np.float64(0.1452372897489714), 'attention': np.float64(0.017696665269808655), 'executive function': np.float64(0.017696665269808655), 'spatial awareness': np.float64(0.0072514030275918325), 'sensorimotor function': np.float64(0.0), 'sensorimotor integration': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visual attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'tactile processing': np.float64(0.0), 'visual spatial processing': np.float64(0.0), 'visual integration': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'object recognition': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'primary visual cortex (left visual field)': np.float64(0.0), 'primary visual cortex (right visual field)': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor preparation': np.float64(0.0), 'high-level visual processing': np.float64(0.0), 'conflict monitoring': np.float64(0.0), 'emotion': np.float64(0.0), 'emotional control': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.5236909005055459,0.7313559081527348
23,"[0.281203007518797, 0.2499140006879945, 0.3740601503759398, 0.24353373552458218, 0.3728442728442728]","[np.float64(0.7835568968674057), np.float64(0.7646698969370122), np.float64(0.7383871610845295), np.float64(0.713068181818182), np.float64(0.8105928509154315)]","{np.str_('S1'): np.float64(0.8181818181818182), np.str_('S2'): np.float64(0.15625), np.str_('S3'): np.float64(0.6818181818181818), np.str_('S4'): np.float64(0.04), np.str_('S5'): np.float64(0.18518518518518517), np.str_('S6'): np.float64(0.03125), np.str_('S7'): np.float64(0.6551724137931034)}","{np.str_('S1'): np.int64(22), np.str_('S2'): np.int64(32), np.str_('S3'): np.int64(22), np.str_('S4'): np.int64(25), np.str_('S5'): np.int64(27), np.str_('S6'): np.int64(32), np.str_('S7'): np.int64(29)}","{'language': np.float64(1.2724728478921168), 'language processing': np.float64(0.8804799904468312), 'object recognition': np.float64(0.8459536759646669), 'auditory processing': np.float64(0.7271715477710275), 'sound localization': np.float64(0.690582505730704), 'visual spatial processing': np.float64(0.5745470507128366), 'visual attention': np.float64(0.4655507890127298), 'spatial processing': np.float64(0.4387728720000378), 'sensorimotor integration': np.float64(0.41979393898680567), 'motor planning': np.float64(0.41016130811991386), 'speech perception': np.float64(0.34605617077803014), 'primary visual cortex (left visual field)': np.float64(0.33865111550845245), 'visual integration': np.float64(0.3209638777461579), 'primary visual cortex (right visual field)': np.float64(0.31689866927032856), 'spatial awareness': np.float64(0.31274333272561594), 'tactile processing': np.float64(0.29479312443935474), 'attention': np.float64(0.284998287197593), 'executive function': np.float64(0.284998287197593), 'auditory association': np.float64(0.2816324954803669), 'auditory-motor integration': np.float64(0.20008631865083437), 'primary auditory processing': np.float64(0.19438861303355282), 'working memory': np.float64(0.14338764709472032), 'decision making': np.float64(0.13968733295960417), 'speech and language integration': np.float64(0.13948880694727328), 'emotional control': np.float64(0.13050455850085962), 'conflict monitoring': np.float64(0.12087174414511023), 'motor cortex - movement of right limbs': np.float64(0.06088213850013443), 'high-level visual processing': np.float64(0.05462487862033904), 'somatosensory integration': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor cortex - movement of left limbs': np.float64(0.0), 'motor preparation': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'motor control': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'cognitive control': np.float64(0.0), 'auditory response': np.float64(0.0)}",0.30431103339031723,0.7620549975245122
24,"[0.15625056266767498, 0.0903149138443256, 0.1454151908697363, 0.12442810457516341, 0.17126623376623376]","[np.float64(0.49139036782714945), np.float64(0.5740561056653011), np.float64(0.5476259585454988), np.float64(0.560828798436557), np.float64(0.5884947388180147)]","{np.str_('S1'): np.float64(0.10714285714285714), np.str_('S2'): np.float64(0.0), np.str_('S3'): np.float64(0.25), np.str_('S4'): np.float64(0.3103448275862069), np.str_('S5'): np.float64(0.05555555555555555), np.str_('S6'): np.float64(0.21739130434782608), np.str_('S7'): np.float64(0.18181818181818182)}","{np.str_('S1'): np.int64(28), np.str_('S2'): np.int64(23), np.str_('S3'): np.int64(20), np.str_('S4'): np.int64(29), np.str_('S5'): np.int64(18), np.str_('S6'): np.int64(23), np.str_('S7'): np.int64(22)}","{'object recognition': np.float64(0.858177717635383), 'visual spatial processing': np.float64(0.7769903290738502), 'visual attention': np.float64(0.7549603819992461), 'sound localization': np.float64(0.7472008156390659), 'spatial awareness': np.float64(0.6156554393143681), 'visual integration': np.float64(0.5828738107553371), 'high-level visual processing': np.float64(0.539326982481526), 'sensorimotor integration': np.float64(0.5358920254316341), 'spatial processing': np.float64(0.5004110664321504), 'motor planning': np.float64(0.39808191919593405), 'tactile processing': np.float64(0.3971836879729007), 'decision making': np.float64(0.28924573459137576), 'motor cortex - movement of left limbs': np.float64(0.18898233727628128), 'primary visual cortex (left visual field)': np.float64(0.1334242695964835), 'attention': np.float64(0.07410100929105326), 'auditory-motor integration': np.float64(0.06235736772821392), 'primary visual cortex (right visual field)': np.float64(0.059888664813515374), 'speech perception': np.float64(0.05941741392607264), 'primary auditory processing': np.float64(0.05941741392607264), 'auditory processing': np.float64(0.05941741392607264), 'conflict monitoring': np.float64(0.057560647133735925), 'executive function': np.float64(0.04960914407680654), 'working memory': np.float64(0.038373764755823936), 'emotional control': np.float64(0.019186882377911968), 'visual association area': np.float64(0.0), 'sensorimotor function': np.float64(0.0), 'somatosensory integration': np.float64(0.0), 'speech and language integration': np.float64(0.0), 'spatial attention': np.float64(0.0), 'visual attention and processing': np.float64(0.0), 'visuospatial attention': np.float64(0.0), 'motor preparation': np.float64(0.0), 'multisensory integration': np.float64(0.0), 'language and auditory attention': np.float64(0.0), 'language processing': np.float64(0.0), 'motor control': np.float64(0.0), 'motor cortex - fine motor control (left)': np.float64(0.0), 'motor cortex - fine motor control (right)': np.float64(0.0), 'motor cortex - movement of left side': np.float64(0.0), 'motor cortex - movement of right limbs': np.float64(0.0), 'motor cortex - movement of right side': np.float64(0.0), 'emotion': np.float64(0.0), 'executive control': np.float64(0.0), 'language': np.float64(0.0), 'cognitive control': np.float64(0.0), 'awareness': np.float64(0.0), 'central visual processing': np.float64(0.0), 'auditory response': np.float64(0.0), 'auditory association': np.float64(0.0)}",0.1375350011446268,0.5524791938585041
